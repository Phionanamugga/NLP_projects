{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7219fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Core ML and NLP libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import spacy\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup, pipeline\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Infrastructure and monitoring\n",
    "import redis\n",
    "import boto3\n",
    "from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
    "import pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "\n",
    "# Async and concurrency\n",
    "import aiohttp\n",
    "from redis.asyncio import Redis\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "\n",
    "# Specialized libraries\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from textblob import TextBlob\n",
    "import openai  # For GPT integration\n",
    "import anthropic  # For Claude integration\n",
    "\n",
    "# Database\n",
    "import sqlite3\n",
    "import pymongo\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Boolean, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05bc9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION & DATA MODELS\n",
    "# Database Models\n",
    "from sqlalchemy.orm import declarative_base  # Updated import\n",
    "class SentimentLabel(Enum):\n",
    "    NEGATIVE = 0\n",
    "    NEUTRAL = 1  \n",
    "    POSITIVE = 2\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Central configuration for the entire pipeline\"\"\"\n",
    "    # Model Configuration\n",
    "    foundation_model: str = \"roberta-large\"\n",
    "    custom_model_path: str = \"models/amazon-food-sentiment-v2\"\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # Processing Configuration\n",
    "    max_sequence_length: int = 512\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 8\n",
    "    \n",
    "    # Business Configuration\n",
    "    confidence_threshold: float = 0.85\n",
    "    human_review_threshold: float = 0.7\n",
    "    priority_categories: List[str] = field(default_factory=lambda: [\"organic\", \"baby food\", \"dietary supplements\"])\n",
    "    \n",
    "    # Infrastructure Configuration\n",
    "    redis_url: str = \"redis://localhost:6379\"\n",
    "    model_registry_url: str = \"s3://amazon-ml-models/food-sentiment/\"\n",
    "    feature_store_url: str = \"s3://amazon-feature-store/\"\n",
    "    db_url: str = \"sqlite:///sentiment_pipeline.db\"\n",
    "    \n",
    "    # Monitoring Configuration\n",
    "    metrics_port: int = 8000\n",
    "    alert_thresholds: Dict[str, float] = field(default_factory=lambda: {\n",
    "        \"accuracy_drop\": 0.05,\n",
    "        \"latency_p95\": 200,  # milliseconds\n",
    "        \"error_rate\": 0.01\n",
    "    })\n",
    "\n",
    "class ReviewInput(BaseModel):\n",
    "    \"\"\"Input schema for review processing\"\"\"\n",
    "    review_id: str\n",
    "    user_id: str\n",
    "    product_id: str\n",
    "    text: str\n",
    "    rating: int = Field(ge=1, le=5)\n",
    "    product_category: Optional[str] = None\n",
    "    reviewer_history: Optional[Dict] = None\n",
    "    timestamp: datetime = Field(default_factory=datetime.utcnow)\n",
    "    language: Optional[str] = None\n",
    "    is_verified_purchase: bool = True\n",
    "\n",
    "class SentimentOutput(BaseModel):\n",
    "    \"\"\"Output schema for sentiment analysis\"\"\"\n",
    "    review_id: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "    sentiment_scores: Dict[str, float]\n",
    "    business_impact_score: float\n",
    "    requires_human_review: bool\n",
    "    key_phrases: List[str]\n",
    "    product_specific_insights: Dict[str, Union[str, float]]\n",
    "    processing_metadata: Dict[str, Union[str, float, int]]\n",
    "\n",
    "# Database Models\n",
    "Base = declarative_base()\n",
    "\n",
    "class ReviewRecord(Base):\n",
    "    __tablename__ = \"reviews\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    review_id = Column(String, unique=True, index=True)\n",
    "    user_id = Column(String, index=True)\n",
    "    product_id = Column(String, index=True)\n",
    "    text = Column(Text)\n",
    "    rating = Column(Integer)\n",
    "    product_category = Column(String)\n",
    "    timestamp = Column(DateTime)\n",
    "    is_verified_purchase = Column(Boolean)\n",
    "    \n",
    "class SentimentRecord(Base):\n",
    "    __tablename__ = \"sentiment_results\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    review_id = Column(String, index=True)\n",
    "    sentiment = Column(String)\n",
    "    confidence = Column(Float)\n",
    "    positive_score = Column(Float)\n",
    "    neutral_score = Column(Float)\n",
    "    negative_score = Column(Float)\n",
    "    business_impact_score = Column(Float)\n",
    "    requires_human_review = Column(Boolean)\n",
    "    processing_time = Column(Float)\n",
    "    model_version = Column(String)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5ff4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA QUALITY & PREPROCESSING\n",
    "# ================================\n",
    "\n",
    "class AdvancedDataQualityEngine:\n",
    "    \"\"\"World-class data quality assessment and cleaning\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.quality_model = self._load_quality_model()\n",
    "        self.fake_review_detector = self._load_fake_detector()\n",
    "        self.redis_client = redis.from_url(config.redis_url)\n",
    "        \n",
    "    def assess_review_quality(self, review: ReviewInput) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive quality assessment\"\"\"\n",
    "        text = review.text\n",
    "        \n",
    "        # Basic quality metrics\n",
    "        word_count = len(text.split())\n",
    "        char_count = len(text)\n",
    "        sentence_count = len(list(self.nlp(text).sents))\n",
    "        \n",
    "        # Advanced quality metrics\n",
    "        quality_scores = {\n",
    "            'length_score': self._calculate_length_score(word_count),\n",
    "            'coherence_score': self._calculate_coherence_score(text),\n",
    "            'informativeness_score': self._calculate_informativeness_score(text),\n",
    "            'authenticity_score': self._calculate_authenticity_score(review),\n",
    "            'sentiment_rating_consistency': self._check_sentiment_rating_consistency(text, review.rating),\n",
    "            'grammar_score': self._calculate_grammar_score(text),\n",
    "            'spam_score': self._calculate_spam_score(text),\n",
    "            'duplicate_likelihood': self._calculate_duplicate_likelihood(text, review.user_id)\n",
    "        }\n",
    "        \n",
    "        # Overall quality score\n",
    "        quality_scores['overall_quality'] = np.mean([\n",
    "            quality_scores['length_score'],\n",
    "            quality_scores['coherence_score'], \n",
    "            quality_scores['informativeness_score'],\n",
    "            quality_scores['authenticity_score'],\n",
    "            quality_scores['sentiment_rating_consistency'],\n",
    "            quality_scores['grammar_score'],\n",
    "            (1 - quality_scores['spam_score']),  # Invert spam score\n",
    "            (1 - quality_scores['duplicate_likelihood'])  # Invert duplicate score\n",
    "        ])\n",
    "        \n",
    "        return quality_scores\n",
    "    \n",
    "    def _calculate_length_score(self, word_count: int) -> float:\n",
    "        \"\"\"Optimal review length: 20-200 words\"\"\"\n",
    "        if word_count < 5:\n",
    "            return 0.1\n",
    "        elif word_count < 20:\n",
    "            return word_count / 20 * 0.7\n",
    "        elif word_count <= 200:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return max(0.5, 1.0 - (word_count - 200) / 300)\n",
    "    \n",
    "    def _calculate_coherence_score(self, text: str) -> float:\n",
    "        \"\"\"Measure text coherence using sentence embeddings\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return 0.8  # Single sentence, assume coherent\n",
    "        \n",
    "        # Calculate sentence similarity (simplified)\n",
    "        coherence_scores = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            sent1 = TextBlob(sentences[i])\n",
    "            sent2 = TextBlob(sentences[i + 1])\n",
    "            # Simplified coherence using common words\n",
    "            common_words = set(sent1.words) & set(sent2.words)\n",
    "            coherence = len(common_words) / max(len(sent1.words), len(sent2.words))\n",
    "            coherence_scores.append(coherence)\n",
    "        \n",
    "        return np.mean(coherence_scores) if coherence_scores else 0.5\n",
    "    \n",
    "    def _calculate_informativeness_score(self, text: str) -> float:\n",
    "        \"\"\"Measure how informative the review is\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Look for informative elements\n",
    "        informative_elements = {\n",
    "            'entities': len(doc.ents),\n",
    "            'product_mentions': len([token for token in doc if token.pos_ == \"NOUN\"]),\n",
    "            'descriptive_adjectives': len([token for token in doc if token.pos_ == \"ADJ\"]),\n",
    "            'comparison_words': len([token for token in doc if token.lemma_ in \n",
    "                                   [\"better\", \"worse\", \"best\", \"worst\", \"compare\", \"than\"]])\n",
    "        }\n",
    "        \n",
    "        # Normalize scores\n",
    "        total_tokens = len(doc)\n",
    "        if total_tokens == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        informativeness = (\n",
    "            informative_elements['entities'] / total_tokens * 2 +\n",
    "            informative_elements['product_mentions'] / total_tokens * 1.5 +\n",
    "            informative_elements['descriptive_adjectives'] / total_tokens * 1.2 +\n",
    "            informative_elements['comparison_words'] / total_tokens * 2\n",
    "        )\n",
    "        \n",
    "        return min(1.0, informativeness * 5)  # Scale to 0-1\n",
    "    \n",
    "    def _calculate_authenticity_score(self, review: ReviewInput) -> float:\n",
    "        \"\"\"Detect potentially fake reviews\"\"\"\n",
    "        text = review.text\n",
    "        \n",
    "        # Red flags for fake reviews\n",
    "        red_flags = 0\n",
    "        \n",
    "        # Generic language patterns\n",
    "        generic_phrases = [\n",
    "            \"highly recommend\", \"amazing product\", \"love this\", \"perfect\",\n",
    "            \"exactly what I needed\", \"fast shipping\", \"great value\"\n",
    "        ]\n",
    "        generic_count = sum(1 for phrase in generic_phrases if phrase.lower() in text.lower())\n",
    "        if generic_count > 3:\n",
    "            red_flags += 0.3\n",
    "        \n",
    "        # Excessive punctuation or caps\n",
    "        caps_ratio = sum(1 for c in text if c.isupper()) / len(text) if text else 0\n",
    "        if caps_ratio > 0.3:\n",
    "            red_flags += 0.2\n",
    "            \n",
    "        # Very short or very positive reviews for low ratings (suspicious)\n",
    "        if review.rating <= 2 and len(text.split()) < 10:\n",
    "            red_flags += 0.4\n",
    "            \n",
    "        # Check for verified purchase\n",
    "        if not review.is_verified_purchase:\n",
    "            red_flags += 0.3\n",
    "        \n",
    "        return max(0.0, 1.0 - red_flags)\n",
    "    \n",
    "    def _check_sentiment_rating_consistency(self, text: str, rating: int) -> float:\n",
    "        \"\"\"Check if sentiment matches the rating\"\"\"\n",
    "        # Simple sentiment analysis for consistency check\n",
    "        blob = TextBlob(text)\n",
    "        sentiment_polarity = blob.sentiment.polarity  # -1 to 1\n",
    "        \n",
    "        # Convert rating to expected sentiment\n",
    "        if rating <= 2:\n",
    "            expected_sentiment = -0.5  # Negative\n",
    "        elif rating == 3:\n",
    "            expected_sentiment = 0.0   # Neutral\n",
    "        else:\n",
    "            expected_sentiment = 0.5   # Positive\n",
    "        \n",
    "        # Calculate consistency\n",
    "        sentiment_diff = abs(sentiment_polarity - expected_sentiment)\n",
    "        consistency = max(0.0, 1.0 - sentiment_diff)\n",
    "        \n",
    "        return consistency\n",
    "    \n",
    "    def _calculate_grammar_score(self, text: str) -> float:\n",
    "        \"\"\"Simplified grammar scoring\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Count grammatical elements\n",
    "        total_tokens = len(doc)\n",
    "        if total_tokens == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        proper_sentences = len([sent for sent in doc.sents if len(sent) > 2])\n",
    "        total_sentences = len(list(doc.sents))\n",
    "        \n",
    "        if total_sentences == 0:\n",
    "            return 0.5\n",
    "            \n",
    "        grammar_score = proper_sentences / total_sentences\n",
    "        return grammar_score\n",
    "    \n",
    "    def _calculate_spam_score(self, text: str) -> float:\n",
    "        \"\"\"Detect spam patterns\"\"\"\n",
    "        spam_indicators = [\n",
    "            \"click here\", \"visit our\", \"website\", \"discount\", \"sale\",\n",
    "            \"buy now\", \"limited time\", \"offer\", \"deal\", \"promo\"\n",
    "        ]\n",
    "        \n",
    "        spam_count = sum(1 for indicator in spam_indicators if indicator.lower() in text.lower())\n",
    "        spam_score = min(1.0, spam_count / 3)  # Normalize\n",
    "        \n",
    "        return spam_score\n",
    "    \n",
    "    def _calculate_duplicate_likelihood(self, text: str, user_id: str) -> float:\n",
    "        \"\"\"Detect potential duplicate reviews\"\"\"\n",
    "        # Create text hash for similarity checking\n",
    "        text_hash = hashlib.md5(text.lower().strip().encode()).hexdigest()\n",
    "        \n",
    "        # Check Redis cache for similar reviews from same user\n",
    "        cache_key = f\"user_reviews:{user_id}\"\n",
    "        try:\n",
    "            user_hashes = self.redis_client.smembers(cache_key)\n",
    "            if text_hash.encode() in user_hashes:\n",
    "                return 0.9  # High duplicate likelihood\n",
    "            \n",
    "            # Add current hash to cache (expire after 30 days)\n",
    "            self.redis_client.sadd(cache_key, text_hash)\n",
    "            self.redis_client.expire(cache_key, 30 * 24 * 3600)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Redis error in duplicate detection: {e}\")\n",
    "        \n",
    "        return 0.1  # Low duplicate likelihood\n",
    "    \n",
    "    def _load_quality_model(self):\n",
    "        \"\"\"Load pre-trained quality assessment model\"\"\"\n",
    "        try:\n",
    "            # In production, this would load from model registry\n",
    "            model_path = Path(\"models/quality_model.pkl\")\n",
    "            if model_path.exists():\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load quality model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _load_fake_detector(self):\n",
    "        \"\"\"Load fake review detection model\"\"\"\n",
    "        try:\n",
    "            model_path = Path(\"models/fake_detector.pkl\")\n",
    "            if model_path.exists():\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load fake detector: {e}\")\n",
    "        return None\n",
    "\n",
    "class IntelligentPreprocessor:\n",
    "    \"\"\"Advanced preprocessing with context awareness\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.language_models = self._load_language_models()\n",
    "        \n",
    "    def preprocess_review(self, review: ReviewInput) -> Dict[str, str]:\n",
    "        \"\"\"Comprehensive preprocessing pipeline\"\"\"\n",
    "        text = review.text\n",
    "        \n",
    "        # Language detection\n",
    "        language = self._detect_language(text)\n",
    "        \n",
    "        # Clean and normalize text\n",
    "        cleaned_text = self._clean_text(text)\n",
    "        \n",
    "        # Product-specific preprocessing\n",
    "        product_enhanced_text = self._enhance_with_product_context(\n",
    "            cleaned_text, review.product_category\n",
    "        )\n",
    "        \n",
    "        # Generate multiple representations\n",
    "        return {\n",
    "            'original': text,\n",
    "            'cleaned': cleaned_text,\n",
    "            'enhanced': product_enhanced_text,\n",
    "            'language': language,\n",
    "            'normalized': self._normalize_text(cleaned_text),\n",
    "            'tokenized': self._smart_tokenize(product_enhanced_text)\n",
    "        }\n",
    "    \n",
    "    def _detect_language(self, text: str) -> str:\n",
    "        \"\"\"Robust language detection\"\"\"\n",
    "        try:\n",
    "            lang_probs = detect_langs(text)\n",
    "            if lang_probs[0].prob > 0.8:\n",
    "                return lang_probs[0].lang\n",
    "        except:\n",
    "            pass\n",
    "        return 'en'  # Default to English\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Advanced text cleaning\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Fix common issues\n",
    "        text = text.replace('&amp;', '&')\n",
    "        text = text.replace('&lt;', '<')\n",
    "        text = text.replace('&gt;', '>')\n",
    "        \n",
    "        # Remove excessive punctuation while preserving meaning\n",
    "        text = re.sub(r'[!]{2,}', '!', text)\n",
    "        text = re.sub(r'[?]{2,}', '?', text)\n",
    "        text = re.sub(r'[.]{3,}', '...', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _enhance_with_product_context(self, text: str, product_category: Optional[str]) -> str:\n",
    "        \"\"\"Add product-specific context\"\"\"\n",
    "        if not product_category:\n",
    "            return text\n",
    "            \n",
    "        # Add category-specific context (this would be more sophisticated in production)\n",
    "        category_contexts = {\n",
    "            'organic': 'organic natural healthy',\n",
    "            'baby food': 'safe nutrition infant toddler',\n",
    "            'snacks': 'taste crunch flavor',\n",
    "            'beverages': 'taste refreshing drink'\n",
    "        }\n",
    "        \n",
    "        context = category_contexts.get(product_category.lower(), '')\n",
    "        if context:\n",
    "            return f\"{text} [CONTEXT: {context}]\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize text for consistency\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Expand contractions\n",
    "        contractions = {\n",
    "            \"won't\": \"will not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"n't\": \" not\",\n",
    "            \"'re\": \" are\",\n",
    "            \"'ve\": \" have\",\n",
    "            \"'ll\": \" will\",\n",
    "            \"'d\": \" would\",\n",
    "            \"'m\": \" am\"\n",
    "        }\n",
    "        \n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _smart_tokenize(self, text: str) -> str:\n",
    "        \"\"\"Intelligent tokenization preserving important patterns\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Preserve important phrases and entities\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.ent_type_:  # Keep entities together\n",
    "                tokens.append(token.text)\n",
    "            elif token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:  # Keep content words\n",
    "                tokens.append(token.lemma_)\n",
    "            else:\n",
    "                tokens.append(token.text)\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def _load_language_models(self):\n",
    "        \"\"\"Load language-specific models\"\"\"\n",
    "        return {\n",
    "            'en': 'en_core_web_sm',\n",
    "            'es': 'es_core_news_sm',\n",
    "            # Add more languages as needed\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd38738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED SENTIMENT MODELS\n",
    "# ================================\n",
    "\n",
    "class HybridSentimentModel(nn.Module):\n",
    "    \"\"\"State-of-the-art hybrid sentiment model for food reviews\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Foundation model backbone\n",
    "        self.transformer = AutoModel.from_pretrained(config.foundation_model)\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Multi-head attention for aspect-based sentiment\n",
    "        self.aspect_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Food-specific aspect extractors\n",
    "        self.taste_extractor = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.quality_extractor = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.value_extractor = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.packaging_extractor = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        \n",
    "        # Hierarchical classification\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size + hidden_size // 2 * 4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, 3)  # Negative, Neutral, Positive\n",
    "        )\n",
    "        \n",
    "        # Confidence estimation\n",
    "        self.confidence_estimator = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Business impact predictor\n",
    "        self.business_impact_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_size + 3, hidden_size // 2),  # +3 for sentiment logits\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_aspects=False, return_confidence=True):\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply aspect attention\n",
    "        attn_output, attn_weights = self.aspect_attention(\n",
    "            sequence_output.transpose(0, 1),\n",
    "            sequence_output.transpose(0, 1), \n",
    "            sequence_output.transpose(0, 1)\n",
    "        )\n",
    "        \n",
    "        # Extract food-specific aspects\n",
    "        taste_features = torch.relu(self.taste_extractor(pooled_output))\n",
    "        quality_features = torch.relu(self.quality_extractor(pooled_output))\n",
    "        value_features = torch.relu(self.value_extractor(pooled_output))\n",
    "        packaging_features = torch.relu(self.packaging_extractor(pooled_output))\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([\n",
    "            pooled_output,\n",
    "            taste_features,\n",
    "            quality_features,\n",
    "            value_features,\n",
    "            packaging_features\n",
    "        ], dim=1)\n",
    "        \n",
    "        combined_features = self.dropout(combined_features)\n",
    "        \n",
    "        # Main sentiment classification\n",
    "        sentiment_logits = self.classifier(combined_features)\n",
    "        \n",
    "        results = {'logits': sentiment_logits}\n",
    "        \n",
    "        if return_confidence:\n",
    "            confidence = self.confidence_estimator(pooled_output)\n",
    "            results['confidence'] = confidence\n",
    "        \n",
    "        if return_aspects:\n",
    "            results['aspects'] = {\n",
    "                'taste': taste_features,\n",
    "                'quality': quality_features,\n",
    "                'value': value_features,\n",
    "                'packaging': packaging_features,\n",
    "                'attention_weights': attn_weights\n",
    "            }\n",
    "        \n",
    "        # Business impact prediction\n",
    "        business_input = torch.cat([pooled_output, sentiment_logits], dim=1)\n",
    "        business_impact = self.business_impact_predictor(business_input)\n",
    "        results['business_impact'] = business_impact\n",
    "        \n",
    "        return results\n",
    "\n",
    "class EnsembleOrchestrator:\n",
    "    \"\"\"Orchestrate multiple models for optimal performance\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.models = self._initialize_models()\n",
    "        self.model_weights = self._calculate_optimal_weights()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.foundation_model)\n",
    "        \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Initialize ensemble of specialized models\"\"\"\n",
    "        models = {}\n",
    "        \n",
    "        try:\n",
    "            # Primary hybrid model\n",
    "            models['primary'] = HybridSentimentModel(self.config)\n",
    "            models['primary'].load_state_dict(\n",
    "                torch.load(f\"{self.config.custom_model_path}/primary_model.pt\", \n",
    "                          map_location='cpu')\n",
    "            )\n",
    "            models['primary'].eval()\n",
    "            \n",
    "            # Specialized models for different aspects\n",
    "            models['roberta_large'] = self._load_pretrained_roberta()\n",
    "            models['food_specialist'] = self._load_food_specialist_model()\n",
    "            models['bert_uncased'] = self._load_bert_model()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing models: {e}\")\n",
    "            # Fallback to basic models\n",
    "            models['fallback'] = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "                tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "            )\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    async def predict_ensemble(self, input_data: str, return_individual=False):\n",
    "        \"\"\"Ensemble prediction with intelligent weighting\"\"\"\n",
    "        predictions = {}\n",
    "        confidences = {}\n",
    "        \n",
    "        # Tokenize input\n",
    "        encoded = self.tokenizer(\n",
    "            input_data,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.config.max_sequence_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        for model_name, model in self.models.items():\n",
    "            try:\n",
    "                result = await self._predict_single_model(model, encoded, input_data)\n",
    "                predictions[model_name] = result['prediction']\n",
    "                confidences[model_name] = result['confidence']\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Model {model_name} failed: {e}\")\n",
    "                predictions[model_name] = None\n",
    "                confidences[model_name] = 0.0\n",
    "        \n",
    "        # Intelligent ensemble combination\n",
    "        final_prediction = self._combine_predictions(predictions, confidences)\n",
    "        \n",
    "        if return_individual:\n",
    "            return {\n",
    "                'ensemble_prediction': final_prediction,\n",
    "                'individual_predictions': predictions,\n",
    "                'individual_confidences': confidences\n",
    "            }\n",
    "        \n",
    "        return final_prediction\n",
    "    \n",
    "    def _combine_predictions(self, predictions, confidences):\n",
    "        \"\"\"Intelligently combine predictions based on confidence and model performance\"\"\"\n",
    "        valid_predictions = {k: v for k, v in predictions.items() if v is not None}\n",
    "        \n",
    "        if not valid_predictions:\n",
    "            return {'sentiment': 'neutral', 'confidence': 0.0, 'scores': {'neutral': 1.0}}\n",
    "        \n",
    "        # Weight by confidence and model performance\n",
    "        weighted_scores = defaultdict(float)\n",
    "        total_weight = 0\n",
    "        \n",
    "        for model_name, prediction in valid_predictions.items():\n",
    "            model_weight = self.model_weights.get(model_name, 0.1)\n",
    "            confidence_weight = confidences.get(model_name, 0.1)\n",
    "            \n",
    "            final_weight = model_weight * confidence_weight\n",
    "            \n",
    "            for sentiment, score in prediction.get('scores', {}).items():\n",
    "                weighted_scores[sentiment] += score * final_weight\n",
    "            \n",
    "            total_weight += final_weight\n",
    "        \n",
    "        # Normalize scores\n",
    "        if total_weight > 0:\n",
    "            normalized_scores = {\n",
    "                sentiment: score / total_weight \n",
    "                for sentiment, score in weighted_scores.items()\n",
    "            }\n",
    "        else:\n",
    "            normalized_scores = {'neutral': 1.0}\n",
    "        \n",
    "        # Get final prediction\n",
    "        final_sentiment = max(normalized_scores, key=normalized_scores.get)\n",
    "        final_confidence = normalized_scores[final_sentiment]\n",
    "        \n",
    "        return {\n",
    "            'sentiment': final_sentiment,\n",
    "            'confidence': final_confidence,\n",
    "            'scores': normalized_scores\n",
    "        }\n",
    "    \n",
    "    def _calculate_optimal_weights(self):\n",
    "        \"\"\"Calculate optimal weights for ensemble (would be learned from validation data)\"\"\"\n",
    "        return {\n",
    "            'primary': 0.4,\n",
    "            'roberta_large': 0.25,\n",
    "            'food_specialist': 0.25,\n",
    "            'bert_uncased': 0.1,\n",
    "            'fallback': 0.3\n",
    "        }\n",
    "    \n",
    "    def _load_pretrained_roberta(self):\n",
    "        \"\"\"Load pre-trained RoBERTa model\"\"\"\n",
    "        try:\n",
    "            return pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "                tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load RoBERTa: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_food_specialist_model(self):\n",
    "        \"\"\"Load food-domain specialist model\"\"\"\n",
    "        try:\n",
    "            # This would be a custom trained model for food reviews\n",
    "            model_path = f\"{self.config.custom_model_path}/food_specialist.pt\"\n",
    "            if os.path.exists(model_path):\n",
    "                return torch.load(model_path, map_location='cpu')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load food specialist: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _load_bert_model(self):\n",
    "        \"\"\"Load BERT model\"\"\"\n",
    "        try:\n",
    "            return pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load BERT: {e}\")\n",
    "            return None\n",
    "    \n",
    "    async def _predict_single_model(self, model, encoded_input, raw_text):\n",
    "        \"\"\"Predict using a single model\"\"\"\n",
    "        try:\n",
    "            if hasattr(model, 'forward'):  # PyTorch model\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**encoded_input)\n",
    "                    logits = outputs['logits']\n",
    "                    probabilities = torch.softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # Map to sentiment labels\n",
    "                    sentiment_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "                    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "                    confidence = probabilities.max().item()\n",
    "                    \n",
    "                    scores = {\n",
    "                        'negative': probabilities[0][0].item(),\n",
    "                        'neutral': probabilities[0][1].item(),\n",
    "                        'positive': probabilities[0][2].item()\n",
    "                    }\n",
    "                    \n",
    "                    return {\n",
    "                        'prediction': {\n",
    "                            'sentiment': sentiment_mapping[predicted_class],\n",
    "                            'scores': scores\n",
    "                        },\n",
    "                        'confidence': confidence\n",
    "                    }\n",
    "                    \n",
    "            elif hasattr(model, '__call__'):  # HuggingFace pipeline\n",
    "                result = model(raw_text)\n",
    "                if isinstance(result, list):\n",
    "                    result = result[0]\n",
    "                \n",
    "                # Normalize labels\n",
    "                label_mapping = {\n",
    "                    'NEGATIVE': 'negative',\n",
    "                    'NEUTRAL': 'neutral', \n",
    "                    'POSITIVE': 'positive',\n",
    "                    'LABEL_0': 'negative',\n",
    "                    'LABEL_1': 'neutral',\n",
    "                    'LABEL_2': 'positive'\n",
    "                }\n",
    "                \n",
    "                sentiment = label_mapping.get(result['label'], 'neutral')\n",
    "                confidence = result['score']\n",
    "                \n",
    "                # Create score distribution (simplified)\n",
    "                scores = {'negative': 0.0, 'neutral': 0.0, 'positive': 0.0}\n",
    "                scores[sentiment] = confidence\n",
    "                # Distribute remaining probability\n",
    "                remaining = (1.0 - confidence) / 2\n",
    "                for key in scores:\n",
    "                    if scores[key] == 0.0:\n",
    "                        scores[key] = remaining\n",
    "                \n",
    "                return {\n",
    "                    'prediction': {\n",
    "                        'sentiment': sentiment,\n",
    "                        'scores': scores\n",
    "                    },\n",
    "                    'confidence': confidence\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Model prediction error: {e}\")\n",
    "            \n",
    "        # Fallback prediction\n",
    "        return {\n",
    "            'prediction': {\n",
    "                'sentiment': 'neutral',\n",
    "                'scores': {'negative': 0.33, 'neutral': 0.34, 'positive': 0.33}\n",
    "            },\n",
    "            'confidence': 0.1\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
