{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7219fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Core ML and NLP libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import spacy\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup, pipeline\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Infrastructure and monitoring\n",
    "import redis\n",
    "import boto3\n",
    "from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
    "import pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "\n",
    "# Async and concurrency\n",
    "import aiohttp\n",
    "from redis.asyncio import Redis\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "\n",
    "# Specialized libraries\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from textblob import TextBlob\n",
    "import openai  # For GPT integration\n",
    "import anthropic  # For Claude integration\n",
    "\n",
    "# Database\n",
    "import sqlite3\n",
    "import pymongo\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Boolean, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05bc9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION & DATA MODELS\n",
    "# Database Models\n",
    "from sqlalchemy.orm import declarative_base  # Updated import\n",
    "class SentimentLabel(Enum):\n",
    "    NEGATIVE = 0\n",
    "    NEUTRAL = 1  \n",
    "    POSITIVE = 2\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Central configuration for the entire pipeline\"\"\"\n",
    "    # Model Configuration\n",
    "    foundation_model: str = \"roberta-large\"\n",
    "    custom_model_path: str = \"models/amazon-food-sentiment-v2\"\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # Processing Configuration\n",
    "    max_sequence_length: int = 512\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 8\n",
    "    \n",
    "    # Business Configuration\n",
    "    confidence_threshold: float = 0.85\n",
    "    human_review_threshold: float = 0.7\n",
    "    priority_categories: List[str] = field(default_factory=lambda: [\"organic\", \"baby food\", \"dietary supplements\"])\n",
    "    \n",
    "    # Infrastructure Configuration\n",
    "    redis_url: str = \"redis://localhost:6379\"\n",
    "    model_registry_url: str = \"s3://amazon-ml-models/food-sentiment/\"\n",
    "    feature_store_url: str = \"s3://amazon-feature-store/\"\n",
    "    db_url: str = \"sqlite:///sentiment_pipeline.db\"\n",
    "    \n",
    "    # Monitoring Configuration\n",
    "    metrics_port: int = 8000\n",
    "    alert_thresholds: Dict[str, float] = field(default_factory=lambda: {\n",
    "        \"accuracy_drop\": 0.05,\n",
    "        \"latency_p95\": 200,  # milliseconds\n",
    "        \"error_rate\": 0.01\n",
    "    })\n",
    "\n",
    "class ReviewInput(BaseModel):\n",
    "    \"\"\"Input schema for review processing\"\"\"\n",
    "    review_id: str\n",
    "    user_id: str\n",
    "    product_id: str\n",
    "    text: str\n",
    "    rating: int = Field(ge=1, le=5)\n",
    "    product_category: Optional[str] = None\n",
    "    reviewer_history: Optional[Dict] = None\n",
    "    timestamp: datetime = Field(default_factory=datetime.utcnow)\n",
    "    language: Optional[str] = None\n",
    "    is_verified_purchase: bool = True\n",
    "\n",
    "class SentimentOutput(BaseModel):\n",
    "    \"\"\"Output schema for sentiment analysis\"\"\"\n",
    "    review_id: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "    sentiment_scores: Dict[str, float]\n",
    "    business_impact_score: float\n",
    "    requires_human_review: bool\n",
    "    key_phrases: List[str]\n",
    "    product_specific_insights: Dict[str, Union[str, float]]\n",
    "    processing_metadata: Dict[str, Union[str, float, int]]\n",
    "\n",
    "# Database Models\n",
    "Base = declarative_base()\n",
    "\n",
    "class ReviewRecord(Base):\n",
    "    __tablename__ = \"reviews\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    review_id = Column(String, unique=True, index=True)\n",
    "    user_id = Column(String, index=True)\n",
    "    product_id = Column(String, index=True)\n",
    "    text = Column(Text)\n",
    "    rating = Column(Integer)\n",
    "    product_category = Column(String)\n",
    "    timestamp = Column(DateTime)\n",
    "    is_verified_purchase = Column(Boolean)\n",
    "    \n",
    "class SentimentRecord(Base):\n",
    "    __tablename__ = \"sentiment_results\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    review_id = Column(String, index=True)\n",
    "    sentiment = Column(String)\n",
    "    confidence = Column(Float)\n",
    "    positive_score = Column(Float)\n",
    "    neutral_score = Column(Float)\n",
    "    negative_score = Column(Float)\n",
    "    business_impact_score = Column(Float)\n",
    "    requires_human_review = Column(Boolean)\n",
    "    processing_time = Column(Float)\n",
    "    model_version = Column(String)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5ff4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA QUALITY & PREPROCESSING\n",
    "# ================================\n",
    "\n",
    "class AdvancedDataQualityEngine:\n",
    "    \"\"\"World-class data quality assessment and cleaning\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.quality_model = self._load_quality_model()\n",
    "        self.fake_review_detector = self._load_fake_detector()\n",
    "        self.redis_client = redis.from_url(config.redis_url)\n",
    "        \n",
    "    def assess_review_quality(self, review: ReviewInput) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive quality assessment\"\"\"\n",
    "        text = review.text\n",
    "        \n",
    "        # Basic quality metrics\n",
    "        word_count = len(text.split())\n",
    "        char_count = len(text)\n",
    "        sentence_count = len(list(self.nlp(text).sents))\n",
    "        \n",
    "        # Advanced quality metrics\n",
    "        quality_scores = {\n",
    "            'length_score': self._calculate_length_score(word_count),\n",
    "            'coherence_score': self._calculate_coherence_score(text),\n",
    "            'informativeness_score': self._calculate_informativeness_score(text),\n",
    "            'authenticity_score': self._calculate_authenticity_score(review),\n",
    "            'sentiment_rating_consistency': self._check_sentiment_rating_consistency(text, review.rating),\n",
    "            'grammar_score': self._calculate_grammar_score(text),\n",
    "            'spam_score': self._calculate_spam_score(text),\n",
    "            'duplicate_likelihood': self._calculate_duplicate_likelihood(text, review.user_id)\n",
    "        }\n",
    "        \n",
    "        # Overall quality score\n",
    "        quality_scores['overall_quality'] = np.mean([\n",
    "            quality_scores['length_score'],\n",
    "            quality_scores['coherence_score'], \n",
    "            quality_scores['informativeness_score'],\n",
    "            quality_scores['authenticity_score'],\n",
    "            quality_scores['sentiment_rating_consistency'],\n",
    "            quality_scores['grammar_score'],\n",
    "            (1 - quality_scores['spam_score']),  # Invert spam score\n",
    "            (1 - quality_scores['duplicate_likelihood'])  # Invert duplicate score\n",
    "        ])\n",
    "        \n",
    "        return quality_scores\n",
    "    \n",
    "    def _calculate_length_score(self, word_count: int) -> float:\n",
    "        \"\"\"Optimal review length: 20-200 words\"\"\"\n",
    "        if word_count < 5:\n",
    "            return 0.1\n",
    "        elif word_count < 20:\n",
    "            return word_count / 20 * 0.7\n",
    "        elif word_count <= 200:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return max(0.5, 1.0 - (word_count - 200) / 300)\n",
    "    \n",
    "    def _calculate_coherence_score(self, text: str) -> float:\n",
    "        \"\"\"Measure text coherence using sentence embeddings\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents if len(sent.text.strip()) > 10]\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return 0.8  # Single sentence, assume coherent\n",
    "        \n",
    "        # Calculate sentence similarity (simplified)\n",
    "        coherence_scores = []\n",
    "        for i in range(len(sentences) - 1):\n",
    "            sent1 = TextBlob(sentences[i])\n",
    "            sent2 = TextBlob(sentences[i + 1])\n",
    "            # Simplified coherence using common words\n",
    "            common_words = set(sent1.words) & set(sent2.words)\n",
    "            coherence = len(common_words) / max(len(sent1.words), len(sent2.words))\n",
    "            coherence_scores.append(coherence)\n",
    "        \n",
    "        return np.mean(coherence_scores) if coherence_scores else 0.5\n",
    "    \n",
    "    def _calculate_informativeness_score(self, text: str) -> float:\n",
    "        \"\"\"Measure how informative the review is\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Look for informative elements\n",
    "        informative_elements = {\n",
    "            'entities': len(doc.ents),\n",
    "            'product_mentions': len([token for token in doc if token.pos_ == \"NOUN\"]),\n",
    "            'descriptive_adjectives': len([token for token in doc if token.pos_ == \"ADJ\"]),\n",
    "            'comparison_words': len([token for token in doc if token.lemma_ in \n",
    "                                   [\"better\", \"worse\", \"best\", \"worst\", \"compare\", \"than\"]])\n",
    "        }\n",
    "        \n",
    "        # Normalize scores\n",
    "        total_tokens = len(doc)\n",
    "        if total_tokens == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        informativeness = (\n",
    "            informative_elements['entities'] / total_tokens * 2 +\n",
    "            informative_elements['product_mentions'] / total_tokens * 1.5 +\n",
    "            informative_elements['descriptive_adjectives'] / total_tokens * 1.2 +\n",
    "            informative_elements['comparison_words'] / total_tokens * 2\n",
    "        )\n",
    "        \n",
    "        return min(1.0, informativeness * 5)  # Scale to 0-1\n",
    "    \n",
    "    def _calculate_authenticity_score(self, review: ReviewInput) -> float:\n",
    "        \"\"\"Detect potentially fake reviews\"\"\"\n",
    "        text = review.text\n",
    "        \n",
    "        # Red flags for fake reviews\n",
    "        red_flags = 0\n",
    "        \n",
    "        # Generic language patterns\n",
    "        generic_phrases = [\n",
    "            \"highly recommend\", \"amazing product\", \"love this\", \"perfect\",\n",
    "            \"exactly what I needed\", \"fast shipping\", \"great value\"\n",
    "        ]\n",
    "        generic_count = sum(1 for phrase in generic_phrases if phrase.lower() in text.lower())\n",
    "        if generic_count > 3:\n",
    "            red_flags += 0.3\n",
    "        \n",
    "        # Excessive punctuation or caps\n",
    "        caps_ratio = sum(1 for c in text if c.isupper()) / len(text) if text else 0\n",
    "        if caps_ratio > 0.3:\n",
    "            red_flags += 0.2\n",
    "            \n",
    "        # Very short or very positive reviews for low ratings (suspicious)\n",
    "        if review.rating <= 2 and len(text.split()) < 10:\n",
    "            red_flags += 0.4\n",
    "            \n",
    "        # Check for verified purchase\n",
    "        if not review.is_verified_purchase:\n",
    "            red_flags += 0.3\n",
    "        \n",
    "        return max(0.0, 1.0 - red_flags)\n",
    "    \n",
    "    def _check_sentiment_rating_consistency(self, text: str, rating: int) -> float:\n",
    "        \"\"\"Check if sentiment matches the rating\"\"\"\n",
    "        # Simple sentiment analysis for consistency check\n",
    "        blob = TextBlob(text)\n",
    "        sentiment_polarity = blob.sentiment.polarity  # -1 to 1\n",
    "        \n",
    "        # Convert rating to expected sentiment\n",
    "        if rating <= 2:\n",
    "            expected_sentiment = -0.5  # Negative\n",
    "        elif rating == 3:\n",
    "            expected_sentiment = 0.0   # Neutral\n",
    "        else:\n",
    "            expected_sentiment = 0.5   # Positive\n",
    "        \n",
    "        # Calculate consistency\n",
    "        sentiment_diff = abs(sentiment_polarity - expected_sentiment)\n",
    "        consistency = max(0.0, 1.0 - sentiment_diff)\n",
    "        \n",
    "        return consistency\n",
    "    \n",
    "    def _calculate_grammar_score(self, text: str) -> float:\n",
    "        \"\"\"Simplified grammar scoring\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Count grammatical elements\n",
    "        total_tokens = len(doc)\n",
    "        if total_tokens == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        proper_sentences = len([sent for sent in doc.sents if len(sent) > 2])\n",
    "        total_sentences = len(list(doc.sents))\n",
    "        \n",
    "        if total_sentences == 0:\n",
    "            return 0.5\n",
    "            \n",
    "        grammar_score = proper_sentences / total_sentences\n",
    "        return grammar_score\n",
    "    \n",
    "    def _calculate_spam_score(self, text: str) -> float:\n",
    "        \"\"\"Detect spam patterns\"\"\"\n",
    "        spam_indicators = [\n",
    "            \"click here\", \"visit our\", \"website\", \"discount\", \"sale\",\n",
    "            \"buy now\", \"limited time\", \"offer\", \"deal\", \"promo\"\n",
    "        ]\n",
    "        \n",
    "        spam_count = sum(1 for indicator in spam_indicators if indicator.lower() in text.lower())\n",
    "        spam_score = min(1.0, spam_count / 3)  # Normalize\n",
    "        \n",
    "        return spam_score\n",
    "    \n",
    "    def _calculate_duplicate_likelihood(self, text: str, user_id: str) -> float:\n",
    "        \"\"\"Detect potential duplicate reviews\"\"\"\n",
    "        # Create text hash for similarity checking\n",
    "        text_hash = hashlib.md5(text.lower().strip().encode()).hexdigest()\n",
    "        \n",
    "        # Check Redis cache for similar reviews from same user\n",
    "        cache_key = f\"user_reviews:{user_id}\"\n",
    "        try:\n",
    "            user_hashes = self.redis_client.smembers(cache_key)\n",
    "            if text_hash.encode() in user_hashes:\n",
    "                return 0.9  # High duplicate likelihood\n",
    "            \n",
    "            # Add current hash to cache (expire after 30 days)\n",
    "            self.redis_client.sadd(cache_key, text_hash)\n",
    "            self.redis_client.expire(cache_key, 30 * 24 * 3600)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Redis error in duplicate detection: {e}\")\n",
    "        \n",
    "        return 0.1  # Low duplicate likelihood\n",
    "    \n",
    "    def _load_quality_model(self):\n",
    "        \"\"\"Load pre-trained quality assessment model\"\"\"\n",
    "        try:\n",
    "            # In production, this would load from model registry\n",
    "            model_path = Path(\"models/quality_model.pkl\")\n",
    "            if model_path.exists():\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load quality model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _load_fake_detector(self):\n",
    "        \"\"\"Load fake review detection model\"\"\"\n",
    "        try:\n",
    "            model_path = Path(\"models/fake_detector.pkl\")\n",
    "            if model_path.exists():\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load fake detector: {e}\")\n",
    "        return None\n",
    "\n",
    "class IntelligentPreprocessor:\n",
    "    \"\"\"Advanced preprocessing with context awareness\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.language_models = self._load_language_models()\n",
    "        \n",
    "    def preprocess_review(self, review: ReviewInput) -> Dict[str, str]:\n",
    "        \"\"\"Comprehensive preprocessing pipeline\"\"\"\n",
    "        text = review.text\n",
    "        \n",
    "        # Language detection\n",
    "        language = self._detect_language(text)\n",
    "        \n",
    "        # Clean and normalize text\n",
    "        cleaned_text = self._clean_text(text)\n",
    "        \n",
    "        # Product-specific preprocessing\n",
    "        product_enhanced_text = self._enhance_with_product_context(\n",
    "            cleaned_text, review.product_category\n",
    "        )\n",
    "        \n",
    "        # Generate multiple representations\n",
    "        return {\n",
    "            'original': text,\n",
    "            'cleaned': cleaned_text,\n",
    "            'enhanced': product_enhanced_text,\n",
    "            'language': language,\n",
    "            'normalized': self._normalize_text(cleaned_text),\n",
    "            'tokenized': self._smart_tokenize(product_enhanced_text)\n",
    "        }\n",
    "    \n",
    "    def _detect_language(self, text: str) -> str:\n",
    "        \"\"\"Robust language detection\"\"\"\n",
    "        try:\n",
    "            lang_probs = detect_langs(text)\n",
    "            if lang_probs[0].prob > 0.8:\n",
    "                return lang_probs[0].lang\n",
    "        except:\n",
    "            pass\n",
    "        return 'en'  # Default to English\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Advanced text cleaning\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Fix common issues\n",
    "        text = text.replace('&amp;', '&')\n",
    "        text = text.replace('&lt;', '<')\n",
    "        text = text.replace('&gt;', '>')\n",
    "        \n",
    "        # Remove excessive punctuation while preserving meaning\n",
    "        text = re.sub(r'[!]{2,}', '!', text)\n",
    "        text = re.sub(r'[?]{2,}', '?', text)\n",
    "        text = re.sub(r'[.]{3,}', '...', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _enhance_with_product_context(self, text: str, product_category: Optional[str]) -> str:\n",
    "        \"\"\"Add product-specific context\"\"\"\n",
    "        if not product_category:\n",
    "            return text\n",
    "            \n",
    "        # Add category-specific context (this would be more sophisticated in production)\n",
    "        category_contexts = {\n",
    "            'organic': 'organic natural healthy',\n",
    "            'baby food': 'safe nutrition infant toddler',\n",
    "            'snacks': 'taste crunch flavor',\n",
    "            'beverages': 'taste refreshing drink'\n",
    "        }\n",
    "        \n",
    "        context = category_contexts.get(product_category.lower(), '')\n",
    "        if context:\n",
    "            return f\"{text} [CONTEXT: {context}]\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize text for consistency\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Expand contractions\n",
    "        contractions = {\n",
    "            \"won't\": \"will not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"n't\": \" not\",\n",
    "            \"'re\": \" are\",\n",
    "            \"'ve\": \" have\",\n",
    "            \"'ll\": \" will\",\n",
    "            \"'d\": \" would\",\n",
    "            \"'m\": \" am\"\n",
    "        }\n",
    "        \n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _smart_tokenize(self, text: str) -> str:\n",
    "        \"\"\"Intelligent tokenization preserving important patterns\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Preserve important phrases and entities\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.ent_type_:  # Keep entities together\n",
    "                tokens.append(token.text)\n",
    "            elif token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:  # Keep content words\n",
    "                tokens.append(token.lemma_)\n",
    "            else:\n",
    "                tokens.append(token.text)\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def _load_language_models(self):\n",
    "        \"\"\"Load language-specific models\"\"\"\n",
    "        return {\n",
    "            'en': 'en_core_web_sm',\n",
    "            'es': 'es_core_news_sm',\n",
    "            # Add more languages as needed\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd38738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED SENTIMENT MODELS\n",
    "# ================================\n",
    "\n",
    "class HybridSentimentModel(nn.Module):\n",
    "    \"\"\"State-of-the-art hybrid sentiment model for food reviews\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Foundation model backbone\n",
    "        self.transformer = AutoModel.from_pretrained(config.foundation_model)\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Multi-head attention for aspect-based sentiment\n",
    "        self.aspect_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Food-specific aspect extractors\n",
    "        self.taste_extractor = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.quality_extractor = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.value_extractor = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.packaging_extractor = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        \n",
    "        # Hierarchical classification\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size + hidden_size // 2 * 4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, 3)  # Negative, Neutral, Positive\n",
    "        )\n",
    "        \n",
    "        # Confidence estimation\n",
    "        self.confidence_estimator = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Business impact predictor\n",
    "        self.business_impact_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_size + 3, hidden_size // 2),  # +3 for sentiment logits\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_aspects=False, return_confidence=True):\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply aspect attention\n",
    "        attn_output, attn_weights = self.aspect_attention(\n",
    "            sequence_output.transpose(0, 1),\n",
    "            sequence_output.transpose(0, 1), \n",
    "            sequence_output.transpose(0, 1)\n",
    "        )\n",
    "        \n",
    "        # Extract food-specific aspects\n",
    "        taste_features = torch.relu(self.taste_extractor(pooled_output))\n",
    "        quality_features = torch.relu(self.quality_extractor(pooled_output))\n",
    "        value_features = torch.relu(self.value_extractor(pooled_output))\n",
    "        packaging_features = torch.relu(self.packaging_extractor(pooled_output))\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([\n",
    "            pooled_output,\n",
    "            taste_features,\n",
    "            quality_features,\n",
    "            value_features,\n",
    "            packaging_features\n",
    "        ], dim=1)\n",
    "        \n",
    "        combined_features = self.dropout(combined_features)\n",
    "        \n",
    "        # Main sentiment classification\n",
    "        sentiment_logits = self.classifier(combined_features)\n",
    "        \n",
    "        results = {'logits': sentiment_logits}\n",
    "        \n",
    "        if return_confidence:\n",
    "            confidence = self.confidence_estimator(pooled_output)\n",
    "            results['confidence'] = confidence\n",
    "        \n",
    "        if return_aspects:\n",
    "            results['aspects'] = {\n",
    "                'taste': taste_features,\n",
    "                'quality': quality_features,\n",
    "                'value': value_features,\n",
    "                'packaging': packaging_features,\n",
    "                'attention_weights': attn_weights\n",
    "            }\n",
    "        \n",
    "        # Business impact prediction\n",
    "        business_input = torch.cat([pooled_output, sentiment_logits], dim=1)\n",
    "        business_impact = self.business_impact_predictor(business_input)\n",
    "        results['business_impact'] = business_impact\n",
    "        \n",
    "        return results\n",
    "\n",
    "class EnsembleOrchestrator:\n",
    "    \"\"\"Orchestrate multiple models for optimal performance\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.models = self._initialize_models()\n",
    "        self.model_weights = self._calculate_optimal_weights()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.foundation_model)\n",
    "        \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Initialize ensemble of specialized models\"\"\"\n",
    "        models = {}\n",
    "        \n",
    "        try:\n",
    "            # Primary hybrid model\n",
    "            models['primary'] = HybridSentimentModel(self.config)\n",
    "            models['primary'].load_state_dict(\n",
    "                torch.load(f\"{self.config.custom_model_path}/primary_model.pt\", \n",
    "                          map_location='cpu')\n",
    "            )\n",
    "            models['primary'].eval()\n",
    "            \n",
    "            # Specialized models for different aspects\n",
    "            models['roberta_large'] = self._load_pretrained_roberta()\n",
    "            models['food_specialist'] = self._load_food_specialist_model()\n",
    "            models['bert_uncased'] = self._load_bert_model()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing models: {e}\")\n",
    "            # Fallback to basic models\n",
    "            models['fallback'] = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "                tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "            )\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    async def predict_ensemble(self, input_data: str, return_individual=False):\n",
    "        \"\"\"Ensemble prediction with intelligent weighting\"\"\"\n",
    "        predictions = {}\n",
    "        confidences = {}\n",
    "        \n",
    "        # Tokenize input\n",
    "        encoded = self.tokenizer(\n",
    "            input_data,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.config.max_sequence_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        for model_name, model in self.models.items():\n",
    "            try:\n",
    "                result = await self._predict_single_model(model, encoded, input_data)\n",
    "                predictions[model_name] = result['prediction']\n",
    "                confidences[model_name] = result['confidence']\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Model {model_name} failed: {e}\")\n",
    "                predictions[model_name] = None\n",
    "                confidences[model_name] = 0.0\n",
    "        \n",
    "        # Intelligent ensemble combination\n",
    "        final_prediction = self._combine_predictions(predictions, confidences)\n",
    "        \n",
    "        if return_individual:\n",
    "            return {\n",
    "                'ensemble_prediction': final_prediction,\n",
    "                'individual_predictions': predictions,\n",
    "                'individual_confidences': confidences\n",
    "            }\n",
    "        \n",
    "        return final_prediction\n",
    "    \n",
    "    def _combine_predictions(self, predictions, confidences):\n",
    "        \"\"\"Intelligently combine predictions based on confidence and model performance\"\"\"\n",
    "        valid_predictions = {k: v for k, v in predictions.items() if v is not None}\n",
    "        \n",
    "        if not valid_predictions:\n",
    "            return {'sentiment': 'neutral', 'confidence': 0.0, 'scores': {'neutral': 1.0}}\n",
    "        \n",
    "        # Weight by confidence and model performance\n",
    "        weighted_scores = defaultdict(float)\n",
    "        total_weight = 0\n",
    "        \n",
    "        for model_name, prediction in valid_predictions.items():\n",
    "            model_weight = self.model_weights.get(model_name, 0.1)\n",
    "            confidence_weight = confidences.get(model_name, 0.1)\n",
    "            \n",
    "            final_weight = model_weight * confidence_weight\n",
    "            \n",
    "            for sentiment, score in prediction.get('scores', {}).items():\n",
    "                weighted_scores[sentiment] += score * final_weight\n",
    "            \n",
    "            total_weight += final_weight\n",
    "        \n",
    "        # Normalize scores\n",
    "        if total_weight > 0:\n",
    "            normalized_scores = {\n",
    "                sentiment: score / total_weight \n",
    "                for sentiment, score in weighted_scores.items()\n",
    "            }\n",
    "        else:\n",
    "            normalized_scores = {'neutral': 1.0}\n",
    "        \n",
    "        # Get final prediction\n",
    "        final_sentiment = max(normalized_scores, key=normalized_scores.get)\n",
    "        final_confidence = normalized_scores[final_sentiment]\n",
    "        \n",
    "        return {\n",
    "            'sentiment': final_sentiment,\n",
    "            'confidence': final_confidence,\n",
    "            'scores': normalized_scores\n",
    "        }\n",
    "    \n",
    "    def _calculate_optimal_weights(self):\n",
    "        \"\"\"Calculate optimal weights for ensemble (would be learned from validation data)\"\"\"\n",
    "        return {\n",
    "            'primary': 0.4,\n",
    "            'roberta_large': 0.25,\n",
    "            'food_specialist': 0.25,\n",
    "            'bert_uncased': 0.1,\n",
    "            'fallback': 0.3\n",
    "        }\n",
    "    \n",
    "    def _load_pretrained_roberta(self):\n",
    "        \"\"\"Load pre-trained RoBERTa model\"\"\"\n",
    "        try:\n",
    "            return pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "                tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load RoBERTa: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_food_specialist_model(self):\n",
    "        \"\"\"Load food-domain specialist model\"\"\"\n",
    "        try:\n",
    "            # This would be a custom trained model for food reviews\n",
    "            model_path = f\"{self.config.custom_model_path}/food_specialist.pt\"\n",
    "            if os.path.exists(model_path):\n",
    "                return torch.load(model_path, map_location='cpu')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load food specialist: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _load_bert_model(self):\n",
    "        \"\"\"Load BERT model\"\"\"\n",
    "        try:\n",
    "            return pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load BERT: {e}\")\n",
    "            return None\n",
    "    \n",
    "    async def _predict_single_model(self, model, encoded_input, raw_text):\n",
    "        \"\"\"Predict using a single model\"\"\"\n",
    "        try:\n",
    "            if hasattr(model, 'forward'):  # PyTorch model\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**encoded_input)\n",
    "                    logits = outputs['logits']\n",
    "                    probabilities = torch.softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # Map to sentiment labels\n",
    "                    sentiment_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "                    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "                    confidence = probabilities.max().item()\n",
    "                    \n",
    "                    scores = {\n",
    "                        'negative': probabilities[0][0].item(),\n",
    "                        'neutral': probabilities[0][1].item(),\n",
    "                        'positive': probabilities[0][2].item()\n",
    "                    }\n",
    "                    \n",
    "                    return {\n",
    "                        'prediction': {\n",
    "                            'sentiment': sentiment_mapping[predicted_class],\n",
    "                            'scores': scores\n",
    "                        },\n",
    "                        'confidence': confidence\n",
    "                    }\n",
    "                    \n",
    "            elif hasattr(model, '__call__'):  # HuggingFace pipeline\n",
    "                result = model(raw_text)\n",
    "                if isinstance(result, list):\n",
    "                    result = result[0]\n",
    "                \n",
    "                # Normalize labels\n",
    "                label_mapping = {\n",
    "                    'NEGATIVE': 'negative',\n",
    "                    'NEUTRAL': 'neutral', \n",
    "                    'POSITIVE': 'positive',\n",
    "                    'LABEL_0': 'negative',\n",
    "                    'LABEL_1': 'neutral',\n",
    "                    'LABEL_2': 'positive'\n",
    "                }\n",
    "                \n",
    "                sentiment = label_mapping.get(result['label'], 'neutral')\n",
    "                confidence = result['score']\n",
    "                \n",
    "                # Create score distribution (simplified)\n",
    "                scores = {'negative': 0.0, 'neutral': 0.0, 'positive': 0.0}\n",
    "                scores[sentiment] = confidence\n",
    "                # Distribute remaining probability\n",
    "                remaining = (1.0 - confidence) / 2\n",
    "                for key in scores:\n",
    "                    if scores[key] == 0.0:\n",
    "                        scores[key] = remaining\n",
    "                \n",
    "                return {\n",
    "                    'prediction': {\n",
    "                        'sentiment': sentiment,\n",
    "                        'scores': scores\n",
    "                    },\n",
    "                    'confidence': confidence\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Model prediction error: {e}\")\n",
    "            \n",
    "        # Fallback prediction\n",
    "        return {\n",
    "            'prediction': {\n",
    "                'sentiment': 'neutral',\n",
    "                'scores': {'negative': 0.33, 'neutral': 0.34, 'positive': 0.33}\n",
    "            },\n",
    "            'confidence': 0.1\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dc21c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUSINESS INTELLIGENCE ENGINE\n",
    "# ================================\n",
    "\n",
    "class BusinessIntelligenceEngine:\n",
    "    \"\"\"Advanced business intelligence for food review sentiment\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.product_knowledge = self._load_product_knowledge()\n",
    "        self.business_rules = self._load_business_rules()\n",
    "        \n",
    "    def calculate_business_impact(self, sentiment_result: Dict, review: ReviewInput) -> Dict:\n",
    "        \"\"\"Calculate comprehensive business impact metrics\"\"\"\n",
    "        \n",
    "        # Base impact score from sentiment and confidence\n",
    "        sentiment_multiplier = {\n",
    "            'positive': 1.2,\n",
    "            'neutral': 0.8,\n",
    "            'negative': -1.5\n",
    "        }\n",
    "        \n",
    "        base_impact = (sentiment_result['confidence'] * \n",
    "                      sentiment_multiplier.get(sentiment_result['sentiment'], 0))\n",
    "        \n",
    "        # Product category impact modifiers\n",
    "        category_multiplier = self._get_category_multiplier(review.product_category)\n",
    "        \n",
    "        # User influence factor\n",
    "        user_influence = self._calculate_user_influence(review.reviewer_history)\n",
    "        \n",
    "        # Urgency scoring\n",
    "        urgency_score = self._calculate_urgency(sentiment_result, review)\n",
    "        \n",
    "        # Revenue impact estimation\n",
    "        revenue_impact = self._estimate_revenue_impact(\n",
    "            sentiment_result, review, base_impact\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'overall_impact_score': base_impact * category_multiplier * user_influence,\n",
    "            'revenue_impact_estimate': revenue_impact,\n",
    "            'urgency_score': urgency_score,\n",
    "            'category_impact': category_multiplier,\n",
    "            'user_influence_factor': user_influence,\n",
    "            'recommended_actions': self._generate_action_recommendations(\n",
    "                sentiment_result, review, urgency_score\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def _get_category_multiplier(self, category: Optional[str]) -> float:\n",
    "        \"\"\"Get business impact multiplier for product category\"\"\"\n",
    "        high_impact_categories = {\n",
    "            'baby food': 2.0,      # High sensitivity\n",
    "            'organic': 1.5,        # Premium products\n",
    "            'dietary supplements': 1.8,  # Health implications\n",
    "            'pet food': 1.3        # Pet owner loyalty\n",
    "        }\n",
    "        \n",
    "        return high_impact_categories.get(category.lower() if category else '', 1.0)\n",
    "    \n",
    "    def _calculate_user_influence(self, reviewer_history: Optional[Dict]) -> float:\n",
    "        \"\"\"Calculate user's influence factor\"\"\"\n",
    "        if not reviewer_history:\n",
    "            return 1.0\n",
    "            \n",
    "        # Factors affecting influence\n",
    "        total_reviews = reviewer_history.get('total_reviews', 1)\n",
    "        helpful_votes = reviewer_history.get('helpful_votes', 0)\n",
    "        reviewer_rank = reviewer_history.get('rank', 'regular')\n",
    "        \n",
    "        # Base influence from review count (logarithmic scaling)\n",
    "        review_influence = min(2.0, 1.0 + np.log10(total_reviews) / 2)\n",
    "        \n",
    "        # Helpful votes boost\n",
    "        helpful_boost = min(1.5, 1.0 + helpful_votes / 100)\n",
    "        \n",
    "        # Rank multiplier\n",
    "        rank_multipliers = {\n",
    "            'top_reviewer': 2.0,\n",
    "            'vine_customer': 1.5,\n",
    "            'verified_purchaser': 1.2,\n",
    "            'regular': 1.0\n",
    "        }\n",
    "        rank_multiplier = rank_multipliers.get(reviewer_rank, 1.0)\n",
    "        \n",
    "        return review_influence * helpful_boost * rank_multiplier\n",
    "    \n",
    "    def _calculate_urgency(self, sentiment_result: Dict, review: ReviewInput) -> float:\n",
    "        \"\"\"Calculate urgency score for business response\"\"\"\n",
    "        urgency = 0.0\n",
    "        \n",
    "        # High urgency for negative sentiment with high confidence\n",
    "        if sentiment_result['sentiment'] == 'negative':\n",
    "            urgency += sentiment_result['confidence'] * 0.8\n",
    "        \n",
    "        # Priority categories get higher urgency\n",
    "        if review.product_category in self.config.priority_categories:\n",
    "            urgency += 0.3\n",
    "        \n",
    "        # Recent reviews get higher urgency\n",
    "        hours_since_review = (datetime.utcnow() - review.timestamp).total_seconds() / 3600\n",
    "        if hours_since_review < 24:\n",
    "            urgency += 0.2\n",
    "        elif hours_since_review < 168:  # 1 week\n",
    "            urgency += 0.1\n",
    "            \n",
    "        return min(1.0, urgency)\n",
    "    \n",
    "    def _estimate_revenue_impact(self, sentiment_result: Dict, review: ReviewInput, base_impact: float) -> Dict:\n",
    "        \"\"\"Estimate potential revenue impact\"\"\"\n",
    "        \n",
    "        # Average order value assumptions (would come from actual data)\n",
    "        avg_order_value = 35.0\n",
    "        \n",
    "        # Review visibility factor (how many people see this review)\n",
    "        visibility_factor = self._estimate_review_visibility(review)\n",
    "        \n",
    "        # Conversion impact (how much sentiment affects purchase decisions)\n",
    "        conversion_impact = {\n",
    "            'positive': 0.05,   # 5% increase in conversion\n",
    "            'neutral': 0.0,     # No impact\n",
    "            'negative': -0.15   # 15% decrease in conversion\n",
    "        }.get(sentiment_result['sentiment'], 0.0)\n",
    "        \n",
    "        # Calculate potential impact\n",
    "        potential_customers_influenced = visibility_factor * 100  # per month\n",
    "        revenue_per_customer = avg_order_value\n",
    "        \n",
    "        monthly_impact = (potential_customers_influenced * \n",
    "                         conversion_impact * \n",
    "                         revenue_per_customer * \n",
    "                         sentiment_result['confidence'])\n",
    "        \n",
    "        return {\n",
    "            'monthly_revenue_impact': monthly_impact,\n",
    "            'annual_revenue_impact': monthly_impact * 12,\n",
    "            'potential_customers_influenced': potential_customers_influenced,\n",
    "            'conversion_impact_rate': conversion_impact\n",
    "        }\n",
    "    \n",
    "    def _estimate_review_visibility(self, review: ReviewInput) -> float:\n",
    "        \"\"\"Estimate how many customers will see this review\"\"\"\n",
    "        # Simplified visibility calculation\n",
    "        base_visibility = 50.0  # Base number of views per month\n",
    "        \n",
    "        # Verified purchase reviews get more visibility\n",
    "        if review.is_verified_purchase:\n",
    "            base_visibility *= 1.5\n",
    "            \n",
    "        # Higher ratings get more visibility (simplified)\n",
    "        rating_multiplier = {1: 0.5, 2: 0.7, 3: 0.9, 4: 1.1, 5: 1.3}\n",
    "        base_visibility *= rating_multiplier.get(review.rating, 1.0)\n",
    "        \n",
    "        return base_visibility\n",
    "    \n",
    "    def _generate_action_recommendations(self, sentiment_result: Dict, review: ReviewInput, urgency_score: float) -> List[str]:\n",
    "        \"\"\"Generate actionable business recommendations\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        sentiment = sentiment_result['sentiment']\n",
    "        confidence = sentiment_result['confidence']\n",
    "        \n",
    "        if sentiment == 'negative' and confidence > 0.7:\n",
    "            if urgency_score > 0.8:\n",
    "                actions.append(\"URGENT: Contact customer within 24 hours\")\n",
    "                actions.append(\"Escalate to product quality team\")\n",
    "            else:\n",
    "                actions.append(\"Schedule customer outreach within 3 days\")\n",
    "                \n",
    "            actions.append(\"Investigate product quality issues\")\n",
    "            actions.append(\"Consider product recall if safety concerns\")\n",
    "            \n",
    "        elif sentiment == 'positive' and confidence > 0.8:\n",
    "            actions.append(\"Consider featuring as customer testimonial\")\n",
    "            actions.append(\"Invite customer to loyalty program\")\n",
    "            \n",
    "        if review.product_category in self.config.priority_categories:\n",
    "            actions.append(\"Flag for regulatory compliance review\")\n",
    "            \n",
    "        if urgency_score > 0.5:\n",
    "            actions.append(\"Monitor for similar complaints\")\n",
    "            \n",
    "        return actions\n",
    "    \n",
    "    def _load_product_knowledge(self):\n",
    "        \"\"\"Load product-specific knowledge base\"\"\"\n",
    "        # This would load from a database or knowledge base\n",
    "        return {\n",
    "            'categories': {\n",
    "                'organic': {\n",
    "                    'keywords': ['organic', 'natural', 'certified', 'pesticide-free'],\n",
    "                    'concerns': ['authenticity', 'price', 'taste']\n",
    "                },\n",
    "                'baby food': {\n",
    "                    'keywords': ['baby', 'infant', 'toddler', 'safe', 'nutrition'],\n",
    "                    'concerns': ['safety', 'ingredients', 'allergies']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _load_business_rules(self):\n",
    "        \"\"\"Load business rules and policies\"\"\"\n",
    "        return {\n",
    "            'response_times': {\n",
    "                'urgent': 4,      # hours\n",
    "                'high': 24,       # hours\n",
    "                'medium': 72,     # hours\n",
    "                'low': 168        # hours (1 week)\n",
    "            },\n",
    "            'escalation_thresholds': {\n",
    "                'negative_confidence': 0.8,\n",
    "                'safety_keywords': ['sick', 'poisoned', 'allergic', 'contaminated'],\n",
    "                'legal_keywords': ['lawsuit', 'lawyer', 'sue', 'legal']\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83475a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEY PHRASE EXTRACTION\n",
    "# ================================\n",
    "\n",
    "class KeyPhraseExtractor:\n",
    "    \"\"\"Extract key phrases and insights from reviews\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def extract_key_phrases(self, text: str, product_category: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"Extract meaningful key phrases from review text\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        key_phrases = []\n",
    "        \n",
    "        # Extract noun phrases\n",
    "        noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks \n",
    "                       if len(chunk.text.split()) <= 3 and len(chunk.text) > 3]\n",
    "        \n",
    "        # Extract named entities\n",
    "        entities = [ent.text.lower() for ent in doc.ents \n",
    "                   if ent.label_ in ['PRODUCT', 'ORG', 'MONEY', 'QUANTITY']]\n",
    "        \n",
    "        # Extract adjective-noun combinations\n",
    "        adj_noun_phrases = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'ADJ' and token.head.pos_ == 'NOUN':\n",
    "                phrase = f\"{token.text.lower()} {token.head.text.lower()}\"\n",
    "                adj_noun_phrases.append(phrase)\n",
    "        \n",
    "        # Combine and filter phrases\n",
    "        all_phrases = noun_phrases + entities + adj_noun_phrases\n",
    "        \n",
    "        # Filter by relevance and frequency\n",
    "        phrase_counts = Counter(all_phrases)\n",
    "        \n",
    "        # Get category-specific important phrases\n",
    "        if product_category:\n",
    "            category_phrases = self._get_category_specific_phrases(text, product_category)\n",
    "            all_phrases.extend(category_phrases)\n",
    "        \n",
    "        # Remove common stop phrases\n",
    "        stop_phrases = {'the product', 'this item', 'these products', 'good quality'}\n",
    "        filtered_phrases = [phrase for phrase in phrase_counts.keys() \n",
    "                          if phrase not in stop_phrases and len(phrase) > 3]\n",
    "        \n",
    "        # Sort by importance (simplified scoring)\n",
    "        scored_phrases = [(phrase, self._score_phrase(phrase, text)) \n",
    "                         for phrase in filtered_phrases]\n",
    "        scored_phrases.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return [phrase for phrase, score in scored_phrases[:10]]\n",
    "    \n",
    "    def _get_category_specific_phrases(self, text: str, category: str) -> List[str]:\n",
    "        \"\"\"Extract category-specific important phrases\"\"\"\n",
    "        category_keywords = {\n",
    "            'baby food': ['taste', 'nutrition', 'safety', 'ingredients', 'organic'],\n",
    "            'organic': ['certified', 'natural', 'pesticides', 'quality'],\n",
    "            'snacks': ['flavor', 'crunch', 'texture', 'addictive'],\n",
    "            'beverages': ['refreshing', 'taste', 'carbonation', 'sweetness']\n",
    "        }\n",
    "        \n",
    "        keywords = category_keywords.get(category.lower(), [])\n",
    "        found_phrases = []\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        for keyword in keywords:\n",
    "            if keyword in text_lower:\n",
    "                # Extract context around keyword\n",
    "                import re\n",
    "                pattern = f'.{{0,20}}{keyword}.{{0,20}}'\n",
    "                matches = re.findall(pattern, text_lower)\n",
    "                found_phrases.extend(matches)\n",
    "        \n",
    "        return found_phrases\n",
    "    \n",
    "    def _score_phrase(self, phrase: str, full_text: str) -> float:\n",
    "        \"\"\"Score phrase importance\"\"\"\n",
    "        # Simple scoring based on length and frequency\n",
    "        frequency_score = full_text.lower().count(phrase.lower())\n",
    "        length_score = len(phrase.split()) / 10  # Prefer longer phrases\n",
    "        \n",
    "        # Boost score for emotional words\n",
    "        emotional_words = ['love', 'hate', 'amazing', 'terrible', 'perfect', 'awful']\n",
    "        emotion_boost = sum(1 for word in emotional_words if word in phrase.lower())\n",
    "        \n",
    "        return frequency_score + length_score + emotion_boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e937b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONITORING AND METRICS\n",
    "\n",
    "class MetricsCollector:\n",
    "    \"\"\"Collect and expose system metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Prometheus metrics\n",
    "        self.request_count = Counter('sentiment_requests_total', 'Total sentiment requests')\n",
    "        self.processing_time = Histogram('sentiment_processing_seconds', 'Processing time')\n",
    "        self.model_accuracy = Gauge('sentiment_model_accuracy', 'Current model accuracy')\n",
    "        self.error_rate = Counter('sentiment_errors_total', 'Total errors')\n",
    "        self.confidence_distribution = Histogram('sentiment_confidence', 'Confidence scores')\n",
    "        \n",
    "        # Business metrics\n",
    "        self.sentiment_distribution = Counter('sentiment_labels_total', \n",
    "                                            'Sentiment distribution', ['sentiment'])\n",
    "        self.human_review_rate = Gauge('human_review_rate', 'Rate of human reviews needed')\n",
    "        self.business_impact_score = Histogram('business_impact_score', 'Business impact scores')\n",
    "        \n",
    "    def record_request(self):\n",
    "        \"\"\"Record a new sentiment analysis request\"\"\"\n",
    "        self.request_count.inc()\n",
    "    \n",
    "    def record_processing_time(self, duration: float):\n",
    "        \"\"\"Record processing time\"\"\"\n",
    "        self.processing_time.observe(duration)\n",
    "    \n",
    "    def record_sentiment(self, sentiment: str, confidence: float):\n",
    "        \"\"\"Record sentiment prediction\"\"\"\n",
    "        self.sentiment_distribution.labels(sentiment=sentiment).inc()\n",
    "        self.confidence_distribution.observe(confidence)\n",
    "    \n",
    "    def record_error(self):\n",
    "        \"\"\"Record an error\"\"\"\n",
    "        self.error_rate.inc()\n",
    "    \n",
    "    def record_human_review(self, required: bool):\n",
    "        \"\"\"Record if human review was required\"\"\"\n",
    "        if required:\n",
    "            self.human_review_rate.inc()\n",
    "    \n",
    "    def record_business_impact(self, score: float):\n",
    "        \"\"\"Record business impact score\"\"\"\n",
    "        self.business_impact_score.observe(score)\n",
    "    \n",
    "    def update_model_accuracy(self, accuracy: float):\n",
    "        \"\"\"Update current model accuracy\"\"\"\n",
    "        self.model_accuracy.set(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41236859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# DATABASE LAYER\n",
    "# ================================\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"Handle all database operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.engine = create_engine(config.db_url)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        Session = sessionmaker(bind=self.engine)\n",
    "        self.session = Session()\n",
    "    \n",
    "    def save_review(self, review: ReviewInput) -> int:\n",
    "        \"\"\"Save review to database\"\"\"\n",
    "        try:\n",
    "            db_review = ReviewRecord(\n",
    "                review_id=review.review_id,\n",
    "                user_id=review.user_id,\n",
    "                product_id=review.product_id,\n",
    "                text=review.text,\n",
    "                rating=review.rating,\n",
    "                product_category=review.product_category,\n",
    "                timestamp=review.timestamp,\n",
    "                is_verified_purchase=review.is_verified_purchase\n",
    "            )\n",
    "            self.session.add(db_review)\n",
    "            self.session.commit()\n",
    "            return db_review.id\n",
    "        except Exception as e:\n",
    "            self.session.rollback()\n",
    "            logging.error(f\"Failed to save review: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_sentiment_result(self, result: SentimentOutput, processing_time: float, model_version: str):\n",
    "        \"\"\"Save sentiment analysis result\"\"\"\n",
    "        try:\n",
    "            sentiment_record = SentimentRecord(\n",
    "                review_id=result.review_id,\n",
    "                sentiment=result.sentiment,\n",
    "                confidence=result.confidence,\n",
    "                positive_score=result.sentiment_scores.get('positive', 0.0),\n",
    "                neutral_score=result.sentiment_scores.get('neutral', 0.0),\n",
    "                negative_score=result.sentiment_scores.get('negative', 0.0),\n",
    "                business_impact_score=result.business_impact_score,\n",
    "                requires_human_review=result.requires_human_review,\n",
    "                processing_time=processing_time,\n",
    "                model_version=model_version\n",
    "            )\n",
    "            self.session.add(sentiment_record)\n",
    "            self.session.commit()\n",
    "        except Exception as e:\n",
    "            self.session.rollback()\n",
    "            logging.error(f\"Failed to save sentiment result: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_recent_results(self, limit: int = 100) -> List[Dict]:\n",
    "        \"\"\"Get recent sentiment analysis results\"\"\"\n",
    "        try:\n",
    "            results = self.session.query(SentimentRecord)\\\n",
    "                .order_by(SentimentRecord.created_at.desc())\\\n",
    "                .limit(limit).all()\n",
    "            \n",
    "            return [{\n",
    "                'review_id': r.review_id,\n",
    "                'sentiment': r.sentiment,\n",
    "                'confidence': r.confidence,\n",
    "                'business_impact_score': r.business_impact_score,\n",
    "                'processing_time': r.processing_time,\n",
    "                'created_at': r.created_at\n",
    "            } for r in results]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get recent results: {e}\")\n",
    "            return []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
